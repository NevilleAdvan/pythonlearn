çˆ¬å–ç½‘ç»œä¸­çš„å›¾ç‰‡æ¶‰åŠåˆ°ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. **å‘é€HTTPè¯·æ±‚**ï¼šè·å–ç½‘é¡µå†…å®¹ã€‚
2. **è§£æHTML**ï¼šæå–å›¾ç‰‡çš„URLã€‚
3. **ä¸‹è½½å›¾ç‰‡**ï¼šæ ¹æ®æå–çš„URLä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Pythonçš„`requests`åº“æ¥å‘é€HTTPè¯·æ±‚ï¼Œä½¿ç”¨`BeautifulSoup`åº“æ¥è§£æHTMLï¼Œä½¿ç”¨`os`åº“æ¥åˆ›å»ºç›®å½•å’Œä¿å­˜å›¾ç‰‡ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ä»£ç ï¼Œå±•ç¤ºäº†å¦‚ä½•çˆ¬å–ç½‘é¡µä¸­çš„å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°ï¼š

### ç¤ºä¾‹ä»£ç 

```python
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin

def fetch_html(url):
    response = requests.get(url)
    response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    return response.text

def parse_image_urls(html, base_url):
    soup = BeautifulSoup(html, 'html.parser')
    img_tags = soup.find_all('img')
    img_urls = [urljoin(base_url, img['src']) for img in img_tags if 'src' in img.attrs]
    return img_urls

def download_image(url, folder):
    response = requests.get(url, stream=True)
    response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    filename = os.path.join(folder, url.split('/')[-1])
    with open(filename, 'wb') as file:
        for chunk in response.iter_content(1024):
            file.write(chunk)

def create_folder(folder):
    if not os.path.exists(folder):
        os.makedirs(folder)

def main(url, folder='images'):
    create_folder(folder)
    html = fetch_html(url)
    img_urls = parse_image_urls(html, url)
    for img_url in img_urls:
        try:
            download_image(img_url, folder)
            print(f"Downloaded {img_url}")
        except Exception as e:
            print(f"Failed to download {img_url}: {e}")

if __name__ == "__main__":
    url = "https://example.com"  # æ›¿æ¢ä¸ºä½ è¦çˆ¬å–çš„ç½‘é¡µURL
    main(url)
```

### ä»£ç è§£é‡Š

1. **fetch_html(url)**ï¼š
   - ä½¿ç”¨`requests.get`å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹ã€‚
   - ä½¿ç”¨`response.raise_for_status`æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸã€‚
   - è¿”å›ç½‘é¡µçš„HTMLå†…å®¹ã€‚

2. **parse_image_urls(html, base_url)**ï¼š
   - ä½¿ç”¨`BeautifulSoup`è§£æHTMLå†…å®¹ã€‚
   - æŸ¥æ‰¾æ‰€æœ‰`<img>`æ ‡ç­¾ã€‚
   - æå–`src`å±æ€§å¹¶ä½¿ç”¨`urljoin`å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URLã€‚
   - è¿”å›å›¾ç‰‡URLåˆ—è¡¨ã€‚

3. **download_image(url, folder)**ï¼š
   - ä½¿ç”¨`requests.get`å‘é€HTTPè¯·æ±‚ä¸‹è½½å›¾ç‰‡ã€‚
   - ä½¿ç”¨`response.iter_content`ä»¥æµçš„æ–¹å¼ä¸‹è½½å›¾ç‰‡ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜ã€‚
   - å°†å›¾ç‰‡ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸­ã€‚

4. **create_folder(folder)**ï¼š
   - æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–‡ä»¶å¤¹ã€‚

5. **main(url, folder)**ï¼š
   - åˆ›å»ºä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹ã€‚
   - è·å–ç½‘é¡µHTMLå†…å®¹ã€‚
   - è§£æå›¾ç‰‡URLã€‚
   - ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡ã€‚

### æ³¨æ„äº‹é¡¹

- ç¡®ä¿ä½ éµå®ˆç›®æ ‡ç½‘ç«™çš„`robots.txt`æ–‡ä»¶å’Œä½¿ç”¨æ¡æ¬¾ã€‚
- é¿å…é¢‘ç¹è¯·æ±‚åŒä¸€ç½‘ç«™ï¼Œä»¥å…è¢«å°ç¦IPã€‚
- å¯¹äºå¤§è§„æ¨¡çˆ¬å–ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥IOæ¥æé«˜æ•ˆç‡ã€‚

### ä¾èµ–å®‰è£…

ä½ éœ€è¦å®‰è£…`requests`å’Œ`BeautifulSoup`åº“ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```sh
pip install requests beautifulsoup4
```

å¸Œæœ›è¿™ä¸ªç¤ºä¾‹ä»£ç èƒ½å¸®åŠ©ä½ çˆ¬å–ç½‘ç»œä¸­çš„å›¾ç‰‡ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶æé—®ï¼ğŸ“¸